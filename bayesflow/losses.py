import tensorflow as tfimport numpy as npdef mse(y_true, y_pred):    """    Computes the MSE error between a set of true parameters (as generated from a prior)    and a set of predicted values (as generated by the nn approximator).    :param y_pred: tf.Tensor of shape (batch_size, n_out_dim)        Predicted values    :param y_true: tf.Tensor of shape (batch_size, n_out_dim)        True values    :return: tf.Tensor of shape (,)        Average loss over batch_size    """    square_differences = np.square(y_true - y_pred)    mse = np.mean(square_differences, axis=-1)    return msedef heteroskedastic_loss(y_true, y_pred):    """    Computes the heteroskedastic loss between a set of true parameters (as generated from a prior)    and a set of predicted values (as generated by the nn approximator).    :param y_pred: tf.Tensor of shape (batch_size, n_out_dim)        Predicted values    :param y_true: tf.Tensor of shape (batch_size, n_out_dim)        True values    :return: tf.Tensor of shape (,)        Single scalar value representing the heteroskedastic loss    """    y_mean, y_var = tf.split(y_pred, 2, axis=-1)    logvar = tf.reduce_sum(input_tensor=0.5 * y_var, axis=-1)    squared_error = tf.reduce_sum(input_tensor=0.5 * tf.square(y_true - y_mean) / tf.exp(y_var), axis=-1)    loss = tf.reduce_mean(input_tensor=squared_error + logvar)    return lossdef maximum_likelihood_loss(z, log_det_J, **args):    """    Computes the ML loss as described by Ardizzone et al. (in press).    :param z: tf.Tensor of shape (batch_size, z_dim)        Output of the final CC block f(x; c, W)    :param log_det_J: tf.Tensor of shape (batch_size, )        Log determinant of the jacobian computed the CC block.    :return tf.Tensor of shape (,)        Single scalar Monte-Carlo approximation of E[ ||z||^2 / 2 - log|det(J)| ]    """    return tf.reduce_mean(0.5 * tf.square(tf.norm(z, axis=-1)) - log_det_J)